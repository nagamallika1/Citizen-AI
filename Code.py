# -*- coding: utf-8 -*-
"""citizen-ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mfcfEiyR5XZryYteb0QunRjrWMXV4c_l
"""

!pip install transformers torch gradio accelerate bitsandbytes

# Imports
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ğŸ“¦ CitizenAI Core Class
class CitizenAI:
    def __init__(self):
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.load_model()

    def load_model(self):
        try:
            print("ğŸ”„ Loading AI model...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
            )
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            print("âœ… AI model loaded.")
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("âš ï¸ Falling back to DialoGPT-medium...")
            fallback_model = "microsoft/DialoGPT-medium"
            self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)
            self.model = AutoModelForCausalLM.from_pretrained(fallback_model)
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            print("âœ… Fallback model loaded.")

    def handle_citizen_query(self, query):
        prompt = f"You are an intelligent citizen engagement assistant. The user has the following query:\n\n{query}\n\nProvide a clear, helpful, civic-centered response:"
        response = self.pipeline(prompt)
        result = response[0]['generated_text'].split("response:")[-1].strip()
        return result

    def get_emergency_contacts(self, city):
        prompt = f"You are an AI civic assistant. Provide a list of emergency contact numbers (police, fire, ambulance, disaster helpline) for the city: {city}."
        response = self.pipeline(prompt)
        result = response[0]['generated_text'].split("helpline:")[-1].strip()
        return result

    def get_city_analysis(self, city):
        prompt = f"You are an AI civic analyst. Provide an overall analysis for the city: {city}, covering recent events, safety, infrastructure, and public services."
        response = self.pipeline(prompt)
        result = response[0]['generated_text'].strip()
        return result

# ğŸ–¥ï¸ Gradio Interface Builder
def create_gradio_interface():
    with gr.Blocks(title="Citizen AI - Intelligent Engagement Platform") as app:
        gr.HTML("<h1 style='text-align:center;'>ğŸ™ï¸ Citizen AI</h1><p style='text-align:center;'>AI-powered Intelligent Citizen Engagement Platform</p>")

        with gr.Tabs():
            # ğŸ—£ï¸ Citizen Query Assistant Tab
            with gr.Tab("ğŸ—£ï¸ Ask Citizen AI"):
                citizen_query_input = gr.Textbox(label="Enter your query", lines=6)
                query_btn = gr.Button("Get AI Response")
                query_output = gr.Textbox(label="Citizen AI Response", lines=12)

            # ğŸš¨ Emergency Contact Finder Tab
            with gr.Tab("ğŸš¨ Emergency Contacts"):
                city_input = gr.Textbox(label="Enter your city name", lines=1)
                contact_btn = gr.Button("Get Emergency Contacts")
                contact_output = gr.Textbox(label="Emergency Contact Information", lines=10)

            # ğŸ“Š City Analysis Tab
            with gr.Tab("ğŸ“Š City Analysis"):
                analysis_city_input = gr.Textbox(label="Enter your city name", lines=1)
                analysis_btn = gr.Button("Get City Analysis")
                analysis_output = gr.Textbox(label="City Analysis Report", lines=12)

        # Event Handlers
        # These handlers will now be able to access citizen_ai as it's initialized outside the __main__ block
        def handle_query(query):
            if not query.strip():
                return "â— Please enter a query."
            result = citizen_ai.handle_citizen_query(query)
            return result

        def handle_contacts(city):
            if not city.strip():
                return "â— Please enter a city name."
            result = citizen_ai.get_emergency_contacts(city)
            return result

        def handle_city_analysis(city):
            if not city.strip():
                return "â— Please enter a city name."
            result = citizen_ai.get_city_analysis(city)
            return result

        # Bind Buttons
        query_btn.click(fn=handle_query, inputs=citizen_query_input, outputs=query_output)
        contact_btn.click(fn=handle_contacts, inputs=city_input, outputs=contact_output)
        analysis_btn.click(fn=handle_city_analysis, inputs=analysis_city_input, outputs=analysis_output)

        gr.HTML("<p style='text-align:center; color:gray;'>âš™ï¸ Powered by IBM Granite AI | CitizenAI Platform</p>")

    return app

# ğŸš€ Run Application
# Move the initialization outside the __main__ block
print("ğŸš€ CitizenAI Initializing...")
citizen_ai = CitizenAI()
iface = create_gradio_interface()
print("ğŸŒ Launching CitizenAI with public link...")

iface.launch(share=True)